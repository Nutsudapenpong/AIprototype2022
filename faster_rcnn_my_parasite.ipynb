{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled35.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMbyzwo/eq9tkdNL0p+E9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nutsudapenpong/AIprototype2022/blob/main/faster_rcnn_my_parasite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrQ562eTmJ6i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import andas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchvision #We need a new version of torchvision for this project"
      ],
      "metadata": {
        "id": "-tiOwMz3paxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import functional as FT\n",
        "from torchvision import transforms as T\n",
        "from torch import nn, optim\n",
        "from torch.n import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random.split, Dataset\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumenations as A #our data augmentation library\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "W-4sfNRPpq7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove warnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import time\n",
        "from tqdm import tqdm # progress bar\n",
        "from torchvision.utils import draw_bounding_boxes"
      ],
      "metadata": {
        "id": "B2Mo7Yfnq9PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "EHBd84zdroSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#our dataset is in cocoformat, we will need pypcoco tools\n",
        "!pip install pycocotools\n",
        "from pycocotools.coco import COCO"
      ],
      "metadata": {
        "id": "bLcycAcor11T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will defind our transforms\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "iaRb3iq-sOJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(train=False):\n",
        "  if train:\n",
        "    transform = A.Compose([\n",
        "                           A.Resize(600, 600), #our input size can be 600px\n",
        "                           A.HorizontalFlip(p=0.3),\n",
        "                           A.VerticalFlip(p=0.3),\n",
        "                           A.RandomBrightnessContrast(p=0.1),\n",
        "                           A.ColorJitter(p=0.1),\n",
        "                           ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='coco'))\n",
        "  else:\n",
        "    transform = A.Compose([\n",
        "                           A.Resize(600, 600),\n",
        "                           ToTensorV2()#our input size can be 600px\n",
        "    ], bbox_params=A.BboxParams(format='coco'))\n",
        "  return transform"
      ],
      "metadata": {
        "id": "uOIl66ehscWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AquariumDetection(datasets.VisionDataset):\n",
        "  def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
        "    #the 3 transform parameers are required for datasets.VisionDataset\n",
        "    super().__init__(root, transforms, rasform, target_transform)\n",
        "    self.split = split #train, valid, test\n",
        "    self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) #annotations stored here\n",
        "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "    self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
        "  \n",
        "  def _load_image(self, id: Int):\n",
        "    path = self.coco.loadImgs(id)[0]['file_name']\n",
        "    image = cv2.imread(os.path.join(self.root, self.split, path))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "  def _load_target(self, id):\n",
        "    return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    id = self.ids[index]\n",
        "    image = self._load_image(id)\n",
        "    target = self._load_target(id)\n",
        "    target = copy.deepcopy(self._load_target(id))\n",
        "\n",
        "    boxes = [t['bbox'] + [t['category_id']] for t in target] #required annotation format for albumentations\n",
        "    if self.transforms is not None:\n",
        "      transformed = self.transforms(image=image, bboxes=boxes)\n",
        "\n",
        "    image = transformed['image']\n",
        "    boxes = transformed['bboxes']\n",
        "\n",
        "    new_boxes = [] #convert from xywh to xyxy\n",
        "    for box in boxes:\n",
        "      xmin = box[0]\n",
        "      xmax = xmin + box[2]\n",
        "      ymin = box[1]\n",
        "      ymax = ymin + box[3]\n",
        "      ew_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
        "\n",
        "    targ = {} #here is our transformed target\n",
        "    targ['boxes'] = boxes\n",
        "    targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
        "    targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
        "    targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) #we have a different area\n",
        "    targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
        "    return image.div(255), targ #scale images\n",
        "  def __len__(self):\n",
        "    return len(self.ids)"
      ],
      "metadata": {
        "id": "k5PifQCtt9hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/aquarium-dataset/Aquarium Combined/\""
      ],
      "metadata": {
        "id": "a_OIaofh-DNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load classes\n",
        "coco = COCO(os.path.join(\"dataset_path\", \"train\", \"_annotations.coco.json\"))\n",
        "categories = coco.cats\n",
        "n_classes = len(categories.keys())\n",
        "categories"
      ],
      "metadata": {
        "id": "8vVvHZV_-Q4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [i[1]['name'] for i in categories.item()]\n",
        "classes"
      ],
      "metadata": {
        "id": "PpQAFIn7_tjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))"
      ],
      "metadata": {
        "id": "WiBfZ5JB_6RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets view a sample\n",
        "sample = train_dataset[12]\n",
        "img_int = torch.tensor(sample[0] * 255, dtype=torch.unit8)\n",
        "plt.imshow(draw_bounding_boxes(\n",
        "    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
        ").permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "R1XrPDmEAHGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "id": "txwIbwWjBJBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets load the faster rcn model\n",
        "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features #we need to change the head\n",
        "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
      ],
      "metadata": {
        "id": "hoQacMPeBY1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "sRBQcIBTCOrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "uMUi-0fZCY4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, targets = next(iter(train_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k:v for k, v in t.item()} for t in targets]\n",
        "output = model(images, targets) #just make sure this runs without error"
      ],
      "metadata": {
        "id": "HuI2qwrLCzoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") #use GPU to train"
      ],
      "metadata": {
        "id": "TF0q8RegDcw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =model.to(device)"
      ],
      "metadata": {
        "id": "EA6NrogrDmp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, and optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) #lr_schedule"
      ],
      "metadata": {
        "id": "zsJx__SHDqIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "vrLscy7qVU-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  #lr_scheduler = None\n",
        "  #if epoch == 0:\n",
        "    #warmup_factor = 1.0 / 1000 #do lr warmup\n",
        "    #warmup_iters = min(1000, len(loader) - 1)\n",
        "\n",
        "    #lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters = warmup.iters)\n",
        "  \n",
        "  all_losses = []\n",
        "  all_losses_dict = []\n",
        "\n",
        "  for images, targets in tqdm(loader):\n",
        "    images = list(image.to(device) for image in images) \n",
        "    targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    loss_dict = model(images, targets) #the model computes the loss automatically if we pass in targets\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "    loss_dict_append = {k: v.item() for k, v i loss_dict.items()}\n",
        "    loss_value = losses.item()\n",
        "\n",
        "    all_losses.append(loss_value)\n",
        "    all_losses_dict.append(loss_dict_append)\n",
        "\n",
        "    if not math.isfinite(loss_value):\n",
        "      print(f\"Loss is {loss_value}, stopping training\") #train if loss becomes infinity\n",
        "      print(loss_dict)\n",
        "      sys.exit(1)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #if lr_scheduler is not None:\n",
        "      #lr_scheduler.step()\n",
        "\n",
        "  all_losses_dict = pd.DataFrame(all_losses_dict) #for printing\n",
        "  print(\"Epoch {}, lr: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "      epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "      all_losses_dict['loss_classifier'].mean(),\n",
        "      all_losses_dict['loss_box_reg'].mean(),\n",
        "      all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "      all_losses_dict['loss_objectness'].mean()\n",
        "  ))"
      ],
      "metadata": {
        "id": "NuBFo8WKG6Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_oe_epoch(model, optimizer, train_loader, device, epoch)\n",
        "  #lr_scheduler.step()"
      ],
      "metadata": {
        "id": "gYx6-ZZwPoop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we will watch first epoch to ensure o errors\n",
        "#while it is training, lets write code to see the models preditions\n",
        "model.eval()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Wdx5jCNcQTQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = AquariumDetection(root=dataset_path, split='test', transforms=get_transforms(Fale)) "
      ],
      "metadata": {
        "id": "k9ZLYAZ_R31B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, _ = test_dataset[32]\n",
        "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "with torch.no_grad():\n",
        "  prediction = model([img.to(device)])\n",
        "  pred = prediction[0]"
      ],
      "metadata": {
        "id": "Vxa9Nvx3RqMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14, 10))\n",
        "plt.imshow(draw_bounding_boxes(img_int,\n",
        "                               pred['boxes'][pred['scores'] > 0.8],\n",
        "                               [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
        "                               ).permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "C2ofI6RtTDph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}